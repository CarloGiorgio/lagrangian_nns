{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np # get rid of this eventually\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import argparse\n",
    "from jax import jit\n",
    "from jax.experimental.ode import odeint\n",
    "from functools import partial # reduces arguments to function by making some subset implicit\n",
    "\n",
    "#ML functions\n",
    "from jax.experimental import stax\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "from jax.experimental.ode import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.stax import Dense, Softplus, Tanh, elementwise, Relu\n",
    "\n",
    "\n",
    "sigmoid = jit(lambda x: 1/(1+jnp.exp(-x)))\n",
    "swish = jit(lambda x: x/(1+jnp.exp(-x)))\n",
    "relu3 = jit(lambda x: jnp.clip(x, 0.0, float('inf'))**3)\n",
    "Swish = elementwise(swish)\n",
    "Relu3 = elementwise(relu3)\n",
    "\n",
    "def extended_mlp(args):\n",
    "    act = {\n",
    "        'softplus': [Softplus, Softplus],\n",
    "        'swish': [Swish, Swish],\n",
    "        'tanh': [Tanh, Tanh],\n",
    "        'tanh_relu': [Tanh, Relu],\n",
    "        'soft_relu': [Softplus, Relu],\n",
    "        'relu_relu': [Relu, Relu],\n",
    "        'relu_relu3': [Relu, Relu3],\n",
    "        'relu3_relu': [Relu3, Relu],\n",
    "        'relu_tanh': [Relu, Tanh],\n",
    "    }[args.act]\n",
    "    hidden = args.hidden_dim\n",
    "    output_dim = args.output_dim\n",
    "    nlayers = args.layers\n",
    "    \n",
    "    layers = []\n",
    "    layers.extend([\n",
    "        Dense(hidden),\n",
    "        act[0]\n",
    "    ])\n",
    "    for _ in range(nlayers - 1):\n",
    "        layers.extend([\n",
    "            Dense(hidden),\n",
    "            act[1]\n",
    "        ])\n",
    "        \n",
    "    layers.extend([Dense(output_dim)])\n",
    "    \n",
    "    return stax.serial(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "\n",
    "@jax.jit\n",
    "def qdotdot(q, q_t, conditionals):\n",
    "    #evaluation of the acceleration\n",
    "    g = conditionals\n",
    "    \n",
    "    q_tt = (\n",
    "        g * (1 - q_t**2)**(5./2) / \n",
    "        (1 + 2 * q_t**2)\n",
    "    )\n",
    "    \n",
    "    return q_t, q_tt\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def ofunc(y, t=None):\n",
    "    #derivatives of the function\n",
    "    q = y[::3]\n",
    "    q_t = y[1::3]\n",
    "    g = y[2::3]\n",
    "    \n",
    "    q_t, q_tt = qdotdot(q, q_t, g)\n",
    "    return jnp.stack([q_t, q_tt, jnp.zeros_like(g)]).T.ravel()\n",
    "\n",
    "def hamiltonian_eom(hamiltonian, state, conditionals, t=None):\n",
    "    #Hamiltonian equation of motion\n",
    "    q, q_t = jnp.split(state, 2)\n",
    "    \n",
    "    #Move to canonical coordinates:\n",
    "    p = q_t/(1.0-q_t**2)**(3/2.0)\n",
    "    q = q\n",
    "    \n",
    "    conditionals = conditionals / 10.0 #Normalize\n",
    "    p_t = -jax.grad(hamiltonian, 0)(q, p, conditionals)\n",
    "    #Move back to generalized coordinates:\n",
    "    q_tt = p_t*(1-q_t**2)**(5.0/2)/(1.0+2*q_t**2)\n",
    "    \n",
    "    #Avoid nans by computing q_t afterwards:\n",
    "    q_t = jax.grad(hamiltonian, 1)(q, p, conditionals)\n",
    "    return jnp.concatenate([q_t, q_tt])\n",
    "\n",
    "# replace the lagrangian with a parameteric model\n",
    "def learned_dynamics(params, nn_forward_fn):\n",
    "    @jit\n",
    "    def dynamics(q, p, conditionals):\n",
    "        state = jnp.concatenate([q, p, conditionals])\n",
    "        return jnp.squeeze(nn_forward_fn(params, state), axis=-1)\n",
    "    return dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for the generation of the dataset\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1, 2), backend='cpu')\n",
    "def gen_data(seed, batch, num):\n",
    "    #takes the seed, number of batches and the time spacing\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    q0 = jax.random.uniform(rng, (batch,), minval=-10, maxval=10)\n",
    "    qt0 = jax.random.uniform(rng+1, (batch,), minval=-0.99, maxval=0.99)\n",
    "    g = jax.random.normal(rng+2, (batch,))*10\n",
    "\n",
    "    y0 = jnp.stack([q0, qt0, g]).T.ravel()\n",
    "\n",
    "    yt = odeint(ofunc, y0, jnp.linspace(0, 1, num=num), mxsteps=300)\n",
    "\n",
    "    qall = yt[:, ::3]\n",
    "    qtall = yt[:, 1::3]\n",
    "    gall = yt[:, 2::3]\n",
    "    \n",
    "    return jnp.stack([qall, qtall]).reshape(2, -1).T, gall.reshape(1, -1).T, qdotdot(qall, qtall, gall)[1].reshape(1, -1).T\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def gen_data_batch(seed, batch):\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    q0 = jax.random.uniform(rng, (batch,), minval=-10, maxval=10)\n",
    "    qt0 = (jnp.tanh(jax.random.normal(jax.random.PRNGKey(1), (batch,))*2)*0.99999)#jax.random.uniform(rng+1, (batch,), minval=-1, maxval=1)\n",
    "    g = jax.random.normal(rng+2, (batch,))*10\n",
    "    \n",
    "    return jnp.stack([q0, qt0]).reshape(2, -1).T, g.reshape(1, -1).T, jnp.stack(qdotdot(q0, qt0, g)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstate, cconditionals, ctarget = gen_data_batch(0, 5)\n",
    "cstate, cconditionals, ctarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ObjectView({'dataset_size': 200,\n",
    " 'fps': 10,\n",
    " 'samples': 100,\n",
    " 'num_epochs': 80000,\n",
    " 'seed': 0,\n",
    " 'loss': 'l1',\n",
    " 'act': 'softplus',\n",
    " 'hidden_dim': 500,\n",
    " 'output_dim': 1,\n",
    " 'layers': 4,\n",
    " 'n_updates': 1,\n",
    " 'lr': 0.001,\n",
    " 'lr2': 2e-05,\n",
    " 'dt': 0.1,\n",
    " 'model': 'gln',\n",
    " 'batch_size': 68,\n",
    " 'l2reg': 5.7e-07,\n",
    "})\n",
    "# args = loaded['args']\n",
    "rng = jax.random.PRNGKey(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_params, nn_forward_fn = extended_mlp(args)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "_, init_params = init_random_params(rng, (-1, 3))\n",
    "rng += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_small_loss = np.inf\n",
    "iteration = 0\n",
    "total_epochs = 100\n",
    "minibatch_per = 3000\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "lr = 1e-3 #1e-3\n",
    "\n",
    "final_div_factor=1e4\n",
    "\n",
    "#OneCycleLR:\n",
    "@jax.jit\n",
    "def OneCycleLR(pct):\n",
    "    #Rush it:\n",
    "    start = 0.3 #0.2\n",
    "    pct = pct * (1-start) + start\n",
    "    high, low = lr, lr/final_div_factor\n",
    "    \n",
    "    scale = 1.0 - (jnp.cos(2 * jnp.pi * pct) + 1)/2\n",
    "    \n",
    "    return low + (high - low)*scale\n",
    "    \n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(\n",
    "    OneCycleLR\n",
    ")\n",
    "opt_state = opt_init(init_params)\n",
    "# opt_state = opt_init(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(OneCycleLR(jnp.linspace(0, 1, num=200)))\n",
    "plt.yscale('log')\n",
    "plt.title('lr schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(params, cstate, cconditionals, ctarget):\n",
    "    runner = jax.vmap(\n",
    "        partial(\n",
    "            hamiltonian_eom,\n",
    "            learned_dynamics(params, nn_forward_fn)), (0, 0), 0)\n",
    "    preds = runner(cstate, cconditionals)[:, [0, 1]]\n",
    "    \n",
    "    error = jnp.abs(preds - ctarget)\n",
    "    #Weight additionally by proximity to c!\n",
    "    error_weights = (1 + 1/jnp.sqrt(1.0-cstate[:, [1]]**2))\n",
    "    \n",
    "    return jnp.sum(error * error_weights)*len(preds)/jnp.sum(error_weights)\n",
    "\n",
    "@jax.jit\n",
    "def update_derivative(i, opt_state, cstate, cconditionals, ctarget):\n",
    "    params = get_params(opt_state)\n",
    "    param_update = jax.grad(\n",
    "            lambda *args: loss(*args)/len(cstate),\n",
    "            0\n",
    "        )(params, cstate, cconditionals, ctarget)\n",
    "    params = get_params(opt_state)\n",
    "    return opt_update(i, param_update, opt_state), params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'best_sr_params_hamiltonian_true_canonical.pkl'\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm(range(total_epochs)):\n",
    "        epoch_loss = 0.0\n",
    "        num_samples = 0\n",
    "        batch = 512\n",
    "        ocstate, occonditionals, octarget = gen_data_batch(epoch, minibatch_per*batch)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for minibatch in range(minibatch_per):\n",
    "            fraction = (epoch + minibatch/minibatch_per)/total_epochs\n",
    "            s = np.s_[minibatch*batch:(minibatch+1)*batch]\n",
    "            \n",
    "            cstate, cconditionals, ctarget = ocstate[s], occonditionals[s], octarget[s]\n",
    "            opt_state, params = update_derivative(fraction, opt_state, cstate, cconditionals, ctarget);\n",
    "            rng += 10\n",
    "            \n",
    "            cur_loss = loss(params, cstate, cconditionals, ctarget)\n",
    "            \n",
    "            epoch_loss += cur_loss\n",
    "            num_samples += len(cstate)\n",
    "        closs = epoch_loss/num_samples\n",
    "        print('epoch={} lr={} loss={}'.format(\n",
    "            epoch, OneCycleLR(fraction), closs)\n",
    "            )\n",
    "        if closs < best_loss:\n",
    "            best_loss = closs\n",
    "            best_params = [[copy(jax.device_get(l2)) for l2 in l1] if len(l1) > 0 else () for l1 in params]\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pkl.dump({'params': best_params, 'description': 'q and g are divided by 10. hidden=500. act=Softplus'},\n",
    "             open(filename, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = (\n",
    "    pkl.load(open('best_sr_params_hamiltonian_true_canonical.pkl', 'rb'))['params']\n",
    ")\n",
    "opt_state = opt_init(best_params)\n",
    "params = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4*1, 4*1), sharex=True, sharey=True)\n",
    "ax_idx = [(i, j) for i in range(1) for j in range(1)]\n",
    "\n",
    "for i in tqdm(range(1)):\n",
    "    \n",
    "    ci = ax_idx[i]\n",
    "    \n",
    "    cstate, cconditionals, ctarget = gen_data((i+4)*(i+1), 1, 50)\n",
    "    \n",
    "    runner = jax.jit(jax.vmap(\n",
    "        partial(\n",
    "            hamiltonian_eom,\n",
    "            learned_dynamics(params, nn_forward_fn)), (0, 0), 0))\n",
    "\n",
    "    @jax.jit\n",
    "    def odefunc_learned(y, t):\n",
    "        return jnp.concatenate((runner(y[None, :2], y[None, [2]])[0], jnp.zeros(1)))\n",
    "\n",
    "    yt_learned = odeint(\n",
    "        odefunc_learned,\n",
    "        jnp.concatenate([cstate[0], cconditionals[0]]),\n",
    "        np.linspace(0, 1, 50),\n",
    "        mxsteps=100)\n",
    "    \n",
    "    cax = ax#[ci[0], ci[1]]\n",
    "    cax.plot(cstate[:, 1], label='Truth')\n",
    "    cax.plot(yt_learned[:, 1], label='Learned')\n",
    "    cax.legend()\n",
    "    if ci[1] == 0:\n",
    "        cax.set_ylabel('Velocity of particle/Speed of light')\n",
    "    if ci[0] == 0:\n",
    "        cax.set_xlabel('Time')\n",
    "        \n",
    "    cax.set_ylim(-1, 1)\n",
    "    \n",
    "plt.title(\"Hamiltonian NN - Special Relativity\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('sr_hnn_true.png', dpi=150)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
